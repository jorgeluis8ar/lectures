\documentclass[
  shownotes,
  xcolor={svgnames},
  hyperref={colorlinks,citecolor=DarkBlue,linkcolor=DarkRed,urlcolor=DarkBlue}
  , aspectratio=169]{beamer}
\usepackage{animate}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{mathpazo}
%\usepackage{xcolor}
\usepackage{multimedia}
\usepackage{fancybox}
\usepackage[para]{threeparttable}
\usepackage{multirow}
\setcounter{MaxMatrixCols}{30}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{lscape}
\usepackage[compatibility=false,font=small]{caption}
\usepackage{booktabs}
\usepackage{ragged2e}
\usepackage{chronosys}
\usepackage{appendixnumberbeamer}
\usepackage{animate}
\setbeamertemplate{caption}[numbered]
\usepackage{color}
%\usepackage{times}
\usepackage{tikz}
\usepackage{comment} %to comment
%% BibTeX settings
\usepackage{natbib}
\bibliographystyle{apalike}
\bibpunct{(}{)}{,}{a}{,}{,}
\setbeamertemplate{bibliography item}{[\theenumiv]}

% Defines columns for bespoke tables
\usepackage{array}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}


\usepackage{xfrac}


\usepackage{multicol}
\setlength{\columnsep}{0.5cm}

% Theme and colors
\usetheme{Boadilla}

% I use steel blue and a custom color palette. This defines it.
\definecolor{andesred}{HTML}{af2433}

% Other options
\providecommand{\U}[1]{\protect\rule{.1in}{.1in}}
\usefonttheme{serif}
\setbeamertemplate{itemize items}[default]
\setbeamertemplate{enumerate items}[square]
\setbeamertemplate{section in toc}[circle]

\makeatletter

\definecolor{mybackground}{HTML}{82CAFA}
\definecolor{myforeground}{HTML}{0000A0}

\setbeamercolor{normal text}{fg=black,bg=white}
\setbeamercolor{alerted text}{fg=red}
\setbeamercolor{example text}{fg=black}

\setbeamercolor{background canvas}{fg=myforeground, bg=white}
\setbeamercolor{background}{fg=myforeground, bg=mybackground}

\setbeamercolor{palette primary}{fg=black, bg=gray!30!white}
\setbeamercolor{palette secondary}{fg=black, bg=gray!20!white}
\setbeamercolor{palette tertiary}{fg=white, bg=andesred}

\setbeamercolor{frametitle}{fg=andesred}
\setbeamercolor{title}{fg=andesred}
\setbeamercolor{block title}{fg=andesred}
\setbeamercolor{itemize item}{fg=andesred}
\setbeamercolor{itemize subitem}{fg=andesred}
\setbeamercolor{itemize subsubitem}{fg=andesred}
\setbeamercolor{enumerate item}{fg=andesred}
\setbeamercolor{item projected}{bg=gray!30!white,fg=andesred}
\setbeamercolor{enumerate subitem}{fg=andesred}
\setbeamercolor{section number projected}{bg=gray!30!white,fg=andesred}
\setbeamercolor{section in toc}{fg=andesred}
\setbeamercolor{caption name}{fg=andesred}
\setbeamercolor{button}{bg=gray!30!white,fg=andesred}


\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter

\definecolor{airforceblue}{rgb}{0.36, 0.54, 0.66}

\usepackage{tikz}
% Tikz settings optimized for causal graphs.
\usetikzlibrary{shapes,decorations,arrows,calc,arrows.meta,fit,positioning}
\tikzset{
    -Latex,auto,node distance =1 cm and 1 cm,semithick,
    state/.style ={ellipse, draw, minimum width = 0.7 cm},
    point/.style = {circle, draw, inner sep=0.04cm,fill,node contents={}},
    bidirected/.style={Latex-Latex,dashed},
    el/.style = {inner sep=2pt, align=left, sloped}
}


\makeatother






%%%%%%%%%%%%%%% BEGINS DOCUMENT %%%%%%%%%%%%%%%%%%

\begin{document}
 
\title[Lecture 17]{Lecture 17: \\ Regularization/Shrinkage Methods- Elastic Net  \\ Causal Inference}
\subtitle{Big Data and Machine Learning for Applied Economics \\ Econ 4676}
\date{\today}

\author[Sarmiento-Barbieri]{Ignacio Sarmiento-Barbieri}
\institute[Uniandes]{Universidad de los Andes}


\begin{frame}[noframenumbering]
\maketitle
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%----------------------------------------------------------------------% 

\begin{frame}
\frametitle{Agenda}

\tableofcontents

\end{frame}

%----------------------------------------------------------------------%
\section{Recap }


%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Recap: Regularization}


\begin{itemize}
\item For $\lambda \geq 0$ given, consider minimizing the following objective function
\item Lasso:
\begin{align}
min_{\beta} L(\beta) = \sum_{i=1}^n (y_i-x_i'\beta)^2 + \lambda \sum_{s=2}^p |\beta_s| 
\end{align}
\item Ridge:
\begin{align}
min_{\beta} R(\beta) = \sum_{i=1}^n (y_i-x_i'\beta)^2 + \lambda \sum_{s=2}^p (\beta_s)^2
\end{align}
\end{itemize}

\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Recap: Regularization Demo}

\begin{scriptsize}
   
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Load the required packages}
\KeywordTok{library}\NormalTok{(}\StringTok{"dplyr"}\NormalTok{) }\CommentTok{\#for data wrangling}
\KeywordTok{library}\NormalTok{(}\StringTok{"caret"}\NormalTok{) }\CommentTok{\#ML}

\KeywordTok{data}\NormalTok{(swiss) }\CommentTok{\#loads the data set}

\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{) }\CommentTok{\#set the seed for replication purposes}
\KeywordTok{str}\NormalTok{(swiss) }\CommentTok{\#conmpact display}
\end{Highlighting}
\end{Shaded}
\end{scriptsize}

\begin{tiny}


\begin{verbatim}
## 'data.frame':    47 obs. of  6 variables:
##  $ Fertility       : num  80.2 83.1 92.5 85.8 76.9 76.1 83.8 92.4 82.4 82.9 ...
##  $ Agriculture     : num  17 45.1 39.7 36.5 43.5 35.3 70.2 67.8 53.3 45.2 ...
##  $ Examination     : int  15 6 5 12 17 9 16 14 12 16 ...
##  $ Education       : int  12 9 5 7 15 7 7 8 7 13 ...
##  $ Catholic        : num  9.96 84.84 93.4 33.77 5.16 ...
##  $ Infant.Mortality: num  22.2 22.2 20.2 20.3 20.6 26.6 23.6 24.9 21 24.4 ...
\end{verbatim}
\end{tiny}


\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Recap: Regularization Demo}

\begin{scriptsize}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ols \textless{}{-}}\StringTok{ }\KeywordTok{train}\NormalTok{(Fertility }\OperatorTok{\textasciitilde{}}\StringTok{ }\NormalTok{.,   }\CommentTok{\# model to fit}
                     \DataTypeTok{data =}\NormalTok{ swiss,                        }
                     \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{10}\NormalTok{),     }\CommentTok{\# Method: crossvalidation, 10 folds}
                     \DataTypeTok{method =} \StringTok{"lm"}\NormalTok{)                      }\CommentTok{\# specifying regression model}
\NormalTok{ols}
\end{Highlighting}
\end{Shaded}
\end{scriptsize}
\begin{tiny}
\begin{verbatim}
## Linear Regression 
## 
## 47 samples
##  5 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 42, 42, 42, 42, 42, 44, ... 
## Resampling results:
## 
##   RMSE      Rsquared   MAE    
##   7.424916  0.6922072  6.31218
## 
## Tuning parameter 'intercept' was held constant at a value of TRUE
\end{verbatim}
\end{tiny}
\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Recap: Regularization Demo}

\begin{scriptsize}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lambda \textless{}{-}}\StringTok{ }\DecValTok{10}\OperatorTok{\^{}}\KeywordTok{seq}\NormalTok{(}\OperatorTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DataTypeTok{length =} \DecValTok{100}\NormalTok{)}
\NormalTok{lasso \textless{}{-}}\StringTok{ }\KeywordTok{train}\NormalTok{(}
\NormalTok{  Fertility }\OperatorTok{\textasciitilde{}}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ swiss, }\DataTypeTok{method =} \StringTok{"glmnet"}\NormalTok{,}
  \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{10}\NormalTok{),}
  \DataTypeTok{tuneGrid =} \KeywordTok{expand.grid}\NormalTok{(}\DataTypeTok{alpha =} \DecValTok{1}\NormalTok{, }\DataTypeTok{lambda=}\NormalTok{lambda), }\DataTypeTok{preProcess =} \KeywordTok{c}\NormalTok{(}\StringTok{"center"}\NormalTok{, }\StringTok{"scale"}\NormalTok{)}
\NormalTok{  )}

\NormalTok{lasso}
\end{Highlighting}
\end{Shaded}
\end{scriptsize}
\begin{tiny}


\begin{verbatim}
## glmnet 
## 
## 47 samples
##  5 predictor
## 
## Pre-processing: centered (5), scaled (5) 
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 43, 43, 43, 42, 42, 41, ... 
## Resampling results across tuning parameters:
## 
##  ...
## 
## Tuning parameter 'alpha' was held constant at a value of 1
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were alpha = 1 and lambda = 0.02009233.
\end{verbatim}
\end{tiny}
\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Recap: Regularization Demo}

\begin{scriptsize}


\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ridge \textless{}{-}}\StringTok{ }\KeywordTok{train}\NormalTok{(}
\NormalTok{  Fertility }\OperatorTok{\textasciitilde{}}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ swiss, }\DataTypeTok{method =} \StringTok{"glmnet"}\NormalTok{,}
  \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{10}\NormalTok{),}
  \DataTypeTok{tuneGrid =} \KeywordTok{expand.grid}\NormalTok{(}\DataTypeTok{alpha =} \DecValTok{0}\NormalTok{,}\DataTypeTok{lambda=}\NormalTok{lambda), }\DataTypeTok{preProcess =} \KeywordTok{c}\NormalTok{(}\StringTok{"center"}\NormalTok{, }\StringTok{"scale"}\NormalTok{)}
\NormalTok{  )}
\NormalTok{ridge}
\end{Highlighting}
\end{Shaded}
\end{scriptsize}
\begin{tiny}
\begin{verbatim}
## glmnet 
## 
## 47 samples
##  5 predictor
## 
## Pre-processing: centered (5), scaled (5) 
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 42, 42, 43, 44, 42, 42, ... 
## Resampling results across tuning parameters:
## 
##  ...
## 
## Tuning parameter 'alpha' was held constant at a value of 0
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were alpha = 0 and lambda = 0.7390722.
\end{verbatim}
\end{tiny}
\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Recap: Regularization Demo}




\begin{verbatim}
## 
## Call:
## summary.resamples(object = ., metric = "RMSE")
## 
## Models: ridge, lasso 
## Number of resamples: 10 
## 
## RMSE 
##           Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA's
## ridge 2.615430 4.674108 7.627190 6.923531 8.939798 10.55026    0
## lasso 3.205868 5.553161 5.961622 7.324069 8.587818 13.46074    0
\end{verbatim}

\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Recap: Regularization Demo}

   \begin{figure}[H] \centering
            \captionsetup{justification=centering}
              \includegraphics[scale=0.6]{figures/comp}
 \end{figure}

\end{frame}
%----------------------------------------------------------------------%
\section{More predictors than observations ($k>n$)}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{More predictors than observations ($k>n$)}

\begin{itemize}
  \item Objective 1: Accuracy
  \begin{itemize}
    \item Minimize prediction error (in one step) $\rightarrow$ Ridge, Lasso
    \end{itemize}
    \bigskip
  \item Objective 2: Dimensionality
  \begin{itemize}
    \item Reduce the predictor space $\rightarrow$ Lasso's free lunch
    \end{itemize}
\end{itemize}
\bigskip
\bigskip
\begin{itemize}
  \item What happens when we have more predictors than observations ($k>n$)?
  \begin{itemize}
   \item OLS fails
   \medskip
   \item Ridge and Lasso to the rescue?
  \end{itemize}
\end{itemize}

\end{frame}

%----------------------------------------------------------------------%
\subsection{OLS with more predictors than observations}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{OLS when $k>n$}

\begin{itemize}
  \item Rank? Max number of rows or columns that are linearly independent
  \begin{itemize}
  \item Implies $rank(X_{k\times n}) \leq min(k,n)$
  \end{itemize}
  \bigskip
  \item MCO we need $rank(X_{k\times n})=k \implies k\leq n$
  \bigskip
  \item If $rank(X_{k\times n})=k$ then $rank(X'X)=k$
  \bigskip
  \item If $k>n$, then  $rank(X'X)\leq n < k$ then $(X'X)$ cannot be inverted
  \bigskip
  \item Ridge and Lasso work when $k \geq n$
\end{itemize}

\end{frame}

%----------------------------------------------------------------------%
\subsection{Lasso and Ridge with $k>n$ }
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Ridge when $k>n$}

\begin{align}
min_{\beta} R(\beta) = \sum_{i=1}^n (y_i-x_i'\beta)^2 + \lambda \sum_{s=2}^p (\beta_s)^2
\end{align}

\bigskip
\begin{itemize}
  \item Solution $\rightarrow$ data augmentation
  \medskip
  \item Problem set 2 HW
  \medskip
  \item Intuition: Ridge ``adds'' $k$ additional points.
  \medskip
  \item Allows us to ``deal'' with $k\geq n$
\end{itemize}

\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Lasso when $k>n$}

\begin{itemize}
  \item Lasso works fine in this case
  \medskip
  \item However, there are some issues to keep in mind
  \begin{itemize}
    \medskip
  \item When $k>n$ chooses at most $n$ variables
  \medskip
  \item When we have a group of highly correlated variables, 
  \medskip
    \begin{itemize}
      \item Lasso chooses only one. Makes it unstable for prediction. (Doesn't happen to Ridge)
      \medskip
      \item Ridge shrinks the coefficients of correlated variables toward each other. This makes Ridge ``work'' better than Lasso. ``Work'' in terms of prediction error
    \end{itemize}
  \end{itemize}  
  \medskip
  
\end{itemize}



\end{frame}
%----------------------------------------------------------------------%
\section{Elastic Net}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Naive Elastic Net}

\begin{itemize}
\item Elastic net: happy medium. 
  \begin{itemize}
    \item Good job at prediction and selecting variables
  \end{itemize}
\end{itemize}

\begin{align}
min_{\beta} NEL(\beta) &= \sum_{i=1}^n (y_i-x_i'\beta)^2 + \lambda_1 \sum_{s=2}^p |\beta_s| + \lambda_2 \sum_{s=2}^p \beta_s^2 
\end{align}


\begin{itemize}
 \item Mixes Ridge and Lasso
 \item Lasso selects predictors
 \item Strict convexity part  of the penalty (ridge) solves the grouping instability problem 
 \scriptsize
 \item H.W.: $\beta_{OLS}>0$ one predictor standarized
 \begin{align}
\hat{\beta}_{naive\,EN}= \frac{\left(\hat{\beta}_{OLS}-\frac{\lambda_1}{2}\right)_{+}}{1+\lambda_2}
\end{align}
\end{itemize}

%The second term encourages highly correlated features to be averaged, while the first term encourages a sparse solution in the coefficients of these averaged features. The elastic net penalty can be used with any linear model, in particular for regression or classification. pg 662 Elements
\end{frame}

%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Elastic Net}

\begin{itemize}
\item Elastic Net: reescaled version
\item Double Shrinkage introduces ``too'' much bias, {\it final} version ``corrects'' for this
\end{itemize}
\bigskip
\begin{align}
\hat{\beta}_{EN}= \frac{1}{\sqrt{1+\lambda_2}}\hat{\beta}_{naive\,EN}
\end{align}
\bigskip
\begin{itemize}
  \item Careful sometimes software asks.
  \item How to choose $(\lambda_1,\lambda_2)$? $\rightarrow$ Bidimensional Crossvalidation
  \item Zou, H. \& Hastie, T. (2005)
\end{itemize}

\end{frame}

%----------------------------------------------------------------------%
\section{Lasso for Causality}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Model Selection When the Goal is Causal Inference}
\framesubtitle{Motivation}


\begin{itemize}
\item Up to this point we only cared about prediction
\medskip
\item Can we use some of these models to do causal inference?
\medskip

\item We are going to see how we can adapt regularization (lasso) to do inference
\end{itemize}

\end{frame}

%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Model Selection When the Goal is Causal Inference}

Let's start with the following model
\medskip
\begin{align}
  y_i = \alpha d_i + g(w_i) + \zeta_i
\end{align}

\medskip

were 

\begin{itemize}
\item $d_i$ is the treatment/policy variable of interest, 
\item $w_i$ is a set controls
\item $E[\theta_i|d_i,w_i]=0$
\end{itemize}
\end{frame}

%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Model Selection When the Goal is Causal Inference}
\begin{itemize}
  \item Traditional approach: researcher selects $w_i$, problem is that mistakes can occur.
  \medskip
  \item Same if uses an ``automatic'' model selection approach. It can leave out potentially important variables with small coefficients but non zero coefficients out
  \medskip
  \item The omission of such variables then generally contaminates estimation and inference results based on the selected set of variables. (e.g. OVB)
  \medskip
  \item The validity of this approach is delicate because it relies on perfect model selection.
  \medskip
  \item Because model selection mistakes seem inevitable in realistic settings, it is important to develop inference procedures that are robust to such mistakes.
  \medskip
  \item Solution here: Use Lasso
\end{itemize}


\end{frame}


%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Model Selection When the Goal is Causal Inference}
\begin{itemize}
\item Using Lasso is useful for prediction
\medskip
\item However, naively using Lasso e to draw inferences about model parameters can be problematic.
\medskip
\item Part of the difficulty is that these procedures are designed for prediction, not for inference  
\medskip
\item Leeb and Pötscher 2008 show that methods that tend to do a good job at prediction can lead to incorrect conclusions when inference is the main objective
\medskip
\item This observation suggests that more desirable inference properties may be obtained if one focuses on model selection over the predictive parts of the economic problem
\begin{itemize}
\item The reduced forms and first-stages—rather than using model selection in the structural model directly.
\end{itemize}

\end{itemize}

\end{frame}

%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Approximate sparse models}

\begin{itemize}
  \item Suppose we are interested in the following model
  \begin{align}
  y_i=g(w_i) + \zeta_i
  \end{align}
  with 
  \begin{itemize}
    \item $E(\zeta_i|g(x_i))=0$
    \item $i=1,\dots,n$ are iid
    \item To avoid over-fitting and produce good out of sample forecast we will need to restrict or regularize $g(.)$
    \item The focus here is on regularization that treats $g(w_i)$ as a high-dimensional, approximately linear model:
    \begin{align}
    g(w_i)=\sum_{j=1}^p \beta_j x_{ij} +r_{pi}
    \end{align}
    \item where $p>>n$ and $r_{pi}$ is small enough 
  
  \end{itemize}
\end{itemize}

\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Approximate sparse models}
\begin{align}
    g(w_i)=\sum_{j=1}^p \beta_j x_{ij} +r_{pi}
    \end{align}

\begin{itemize}

    \item Approximate sparsity of this high-dimensional linear model imposes the restriction that linear combinations of only $s<<n$ $x_{ij}$ variables provide a good approximation to $g(w_i)$
    \medskip
    \item A bonus is that the identity of this $s$ $x_{ij}$ variables are a priori unknown
    \medskip
    \item And that we can have a nonzero approximation error $r_{pi}$
    \medskip
    \item We are going to try to learn the identities of these variables while estimating the coefficients.

\end{itemize}

\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Approximate sparse models}
\begin{itemize}

\item We can use Lasso that is slightly modified
\begin{align}
L(\beta) &= \sum_{i=1}^n (y_i-x_i'\beta)^2 + \lambda \sum_{s=2}^p |\beta_s| \gamma_s
\end{align}

\begin{itemize}
  \item where $\lambda>0$ is the penalty level chosen using  Belloni, Chen, Chernozhukov, and Hansen (2012)
  \item $\gamma_j$ are {\it penalty loadings}
  \item  {\it penalty loadings} are chosen to insure equivariance of coefficient estimates to rescaling of $x_{ij}$ and can also be chosen to address heteroskedasticity, clustering, and non-gaussian errors

\end{itemize}
\end{itemize}

\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Inference with Selection among Many Controls}

\begin{itemize}
\item Consider a linear model where a treatment variable, $d_i$, is taken as exogenous after conditioning on control variables
\begin{align}
y_i = \alpha d_i + x_i'\theta_y +r_{yi} + \zeta_i
\end{align}

\item where $E[\zeta_i|d_i,x_i,r_{yi}]=0$ 
\item $x_i$ is a p-dimensional vector with $p>>n$
\item $r_{yi}$ is an approximation error
\item the parameter of interest is $\alpha$

\end{itemize}
\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Inference with Selection among Many Controls}


\begin{itemize}
  \item Naive approach (doesn't work)
  \begin{align}
    y_i = \alpha d_i + x_i'\theta_y +r_{yi} + \zeta_i
  \end{align}
  \begin{itemize}
  \item Select control variables by applying Lasso, forcing the treatment variable to remain in the model
  \medskip
  \item  One could then try to estimate and do inference about $\alpha$ by applying ordinary least squares with $y_i$ as the outcome, and $d_i$ and any selected control variables as regressors. 
  \medskip
  \item The problem is that it target prediction $\rightarrow$ any variable that is highly correlated to the treatment variable will tend to be dropped 
  \medskip
  \item  Of course, the exclusion of a variable that is highly correlated to the treatment will lead to substantial omitted-variables bias 
  \end{itemize}
\end{itemize}

\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Inference with Selection among Many Controls}
\begin{itemize}
\item There are problems with the above naive approach. 
\bigskip
\item It ignores a key component to understanding omitted-variables bias, the relationship between the treatment variable and the controls. 
\medskip
\begin{itemize}
  \item To aid in learning about this relationship, we introduce an additional “reduced form” relation between the treatment and controls:
\end{itemize}


\begin{align}
d_i = x_i'\theta_d + r_{di}+v_i
\end{align}
\end{itemize}

 where $E[v_i|x_i,r_{di}]=0$



\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Inference with Selection among Many Controls}
\begin{itemize}
\item The naive approach is based on a “structural” model where the target is to learn the treatment effect given controls, not an equation representing a prediction rule for $y_i$ given $d_i$ and $x_i$. 
\item It is thus useful to transform the first equation of this section to a reduced form, predictive equation by substituting the equation introduced for $d_i$ into the “structural” equation yielding the reduced form system:

\begin{align}
y_i &= x_i'(\alpha \theta_d +  \theta_y) + (\alpha r_{di} + r_{yi}) + r_{di}+ (\alpha v_i +\zeta_i) = x_i' \pi + r_{ci} + \epsilon_i \\ \nonumber
d_i &= x_i' \theta_d + r_{di} + v_i
\end{align}
\item where $E(\epsilon_i | x_i,r_{ci}]=0$ 
\item $r_{ci}$ is a composite approximation error
\item Both of these equations represent predictive relationships, which may be estimated using high-dimensional methods.


\end{itemize}

\end{frame}


%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Inference with Selection among Many Controls}

\begin{itemize}


\item To prevent model selection mistakes, it is important to consider both equations for selection: 
\medskip
\item We apply variable selection methods to each of the two reduced form equations and then use all of the selected controls in estimation of $\alpha$. 
\medskip
\item We select
\begin{enumerate}
\item A set of variables that are useful for predicting $y_i$, say $x_{yi}$, and 
\item A set of variables that are useful for predicting $d_i$, say $x_{di}$.

\end{enumerate}
\item We then estimate $\alpha$ by ordinary least squares regression of $y_i$ on $d_i$ and the union of the variables selected for predicting $y_i$ and $d_i$, contained in $x_{yi}$ and $x_{di}$. 

\item We thus make sure we use variables that are important for either of the two predictive relationships to guard against OVB

%Using both variable selection steps immunizes the resulting procedure against the types of model selection mistakes discussed above for single-equation procedures. Specififically, using the variables selected in both reduced form equations ensures that any variables that have large effects in either the “structural” equation for yi or the reduced form equation for di are included in the model. Any excluded variables are therefore at most mildly associated to yi and di, which greatly limits the scope for omitted-variables bias. It is also noteworthy that the “double selection” procedure implicitly estimates the residuals εi and vi and then regresses the estimates of εi on the estimates of vi to construct an estimator of α, thereby providing a selection analog of Robinson’s (1988) method for estimating the parameters of a partially linear model to the high-dimensional case.
\end{itemize}



\end{frame}

%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Inference with Selection among Many Controls}

   \begin{figure}[H] \centering
            \captionsetup{justification=centering}
              \includegraphics[scale=0.30]{figures/fig1_belloni}
 \end{figure}


\end{frame}
%----------------------------------------------------------------------%
\subsection{Application}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Application: Estimation of the treatment effect in a linear model with many confounding factors}

\begin{itemize}
\item What is the effect of an initial (lagged) level of GDP per capita on the growth rates of GDP per capita?
\medskip
\item Solow-Swan-Ramsey growth model predicts convergence
\medskip
\item Poorer countries should typically grow faster and therefore should tend to catch up with the richer countries, conditional on a set of institutional and societal characteristics. 
\medskip
\item Covariates that describe such characteristics include variables measuring education and science policies, strength of market institutions, trade openness, savings rates and others.
\end{itemize}

\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Application: Estimation of the treatment effect in a linear model with many confounding factors}


Thus, we are interested in a specification of the form:

\begin{align}
y_i = \alpha d_i+ \sum_{j=1}^p \beta_j x_{ij} + \varepsilon_i
\end{align}

where
\begin{itemize}
\item  \(y_i\) is the growth rate of GDP over a specified decade in country \(i\), 
\item \(d_i\) is the log of the initial level of GDP at the beginning of the specified period, 
\item \(x_{ij}\)'s form a long list of country \(i\)'s characteristics at the beginning of the specified period. 
\item We are interested in testing the hypothesis of convergence,  \(\alpha_1 < 0\).
\end{itemize}


 

\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Application: Estimation of the treatment effect in a linear model with many confounding factors}
For this exercise we use the Barro and Lee (1994) data


\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{require}\NormalTok{(}\StringTok{"hdm"}\NormalTok{) }\CommentTok{\#package}
\KeywordTok{data}\NormalTok{(GrowthData) }\CommentTok{\#load data}
\KeywordTok{dim}\NormalTok{(GrowthData)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 90 63
\end{verbatim}

The number of covariates \(p\) is large relative to the sample size \(n\)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y =}\StringTok{ }\NormalTok{GrowthData[,}\DecValTok{1}\NormalTok{,drop=F]}
\NormalTok{d =}\StringTok{ }\NormalTok{GrowthData[,}\DecValTok{3}\NormalTok{, drop=F]}
\NormalTok{X =}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(GrowthData)[,}\OperatorTok{{-}}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{)]}
\NormalTok{varnames =}\StringTok{ }\KeywordTok{colnames}\NormalTok{(GrowthData)}
\end{Highlighting}
\end{Shaded}

\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Application: Estimation of the treatment effect in a linear model with many confounding factors}

\begin{itemize}
\item Now we can estimate the effect of the initial GDP level. 
\item First, we estimate by OLS:
\end{itemize}


\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{xnames=}\StringTok{ }\NormalTok{varnames[}\OperatorTok{{-}}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{)] }\CommentTok{\# names of X variables}
\NormalTok{dandxnames=}\StringTok{ }\NormalTok{varnames[}\OperatorTok{{-}}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{)] }\CommentTok{\# names of D and X variables}

\CommentTok{\# create formulas by pasting names (this saves typing times)}
\NormalTok{fmla=}\StringTok{ }\KeywordTok{as.formula}\NormalTok{(}\KeywordTok{paste}\NormalTok{(}\StringTok{"Outcome \textasciitilde{} "}\NormalTok{, }\KeywordTok{paste}\NormalTok{(dandxnames, }\DataTypeTok{collapse=} \StringTok{"+"}\NormalTok{)))}

\CommentTok{\# Estimate using OLS}
\NormalTok{ls.effect=}\StringTok{ }\KeywordTok{lm}\NormalTok{(fmla, }\DataTypeTok{data=}\NormalTok{GrowthData)}
\end{Highlighting}
\end{Shaded}

\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Application: Estimation of the treatment effect in a linear model with many confounding factors}

Second, we estimate the effect by the partialling out by Post-Lasso:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dX =}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(}\KeywordTok{cbind}\NormalTok{(d,X))}
\NormalTok{lasso.effect =}\StringTok{ }\KeywordTok{rlassoEffect}\NormalTok{(}\DataTypeTok{x=}\NormalTok{X, }\DataTypeTok{y=}\NormalTok{y, }\DataTypeTok{d=}\NormalTok{d, }\DataTypeTok{method=}\StringTok{"partialling out"}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(lasso.effect)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Estimates and significance testing of the effect of target variables"
##      Estimate. Std. Error t value Pr(>|t|)    
## [1,]  -0.04981    0.01394  -3.574 0.000351 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Application: Estimation of the treatment effect in a linear model with many confounding factors}

Third, we estimate the effect by the double selection method:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dX =}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(}\KeywordTok{cbind}\NormalTok{(d,X))}
\NormalTok{doublesel.effect =}\StringTok{ }\KeywordTok{rlassoEffect}\NormalTok{(}\DataTypeTok{x=}\NormalTok{X, }\DataTypeTok{y=}\NormalTok{y, }\DataTypeTok{d=}\NormalTok{d, }\DataTypeTok{method=}\StringTok{"double selection"}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(doublesel.effect)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Estimates and significance testing of the effect of target variables"
##          Estimate. Std. Error t value Pr(>|t|)   
## gdpsh465  -0.05001    0.01579  -3.167  0.00154 **
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}


\end{frame}
%----------------------------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Application: Estimation of the treatment effect in a linear model with many confounding factors}

\begin{itemize}
\item Collecting the results
\end{itemize}
\bigskip
\bigskip

\begin{table}[ht]
\centering
\begin{tabular}{rrr}
  \hline
 & Estimate & Std. Error \\ 
  \hline
full reg via ols & -0.01 & 0.02989 \\ 
  partial reg
via post-lasso  & -0.05 & 0.01394 \\ 
  partial reg via double selection & -0.05 & 0.01579 \\ 
   \hline
\end{tabular}
\end{table}

\end{frame}
%----------------------------------------------------------------------%
\section{Review \& Next Steps}
%----------------------------------------------------------------------%
\begin{frame}
\frametitle{Review \& Next Steps}
  
\begin{itemize} 
    \item Today:
    \medskip
    \begin{itemize} 
      \item More predictors than observations $(k > n)$
        \begin{itemize}  
            \item OLS doesn't work
            \item Lasso and Ridge work with issues
        \end{itemize}      
     \medskip   
    \item Elastic Net
    \medskip
    \item Lasso for Causality: Post Lasso Double Selection


    \end{itemize}
    \bigskip  
  \item  Next class:  Classification


\bigskip  
\item Questions? Questions about software? 

\end{itemize}
\end{frame}
%----------------------------------------------------------------------%
\section{Further Readings}
%----------------------------------------------------------------------%
\begin{frame}
\frametitle{Further Readings}
\scriptsize
\begin{itemize}

  \item Belloni, A., Chernozhukov, V., \& Hansen, C. (2014). Inference on treatment effects after selection among high-dimensional controls. The Review of Economic Studies, 81(2), 608-650.
  \medskip
  \item Belloni, A., Chernozhukov, V., \& Hansen, C. (2014). High-dimensional methods and inference on structural and treatment effects. Journal of Economic Perspectives, 28(2), 29-50.
  \medskip
  \item Chernozhukov, V.,  Hansen, C., \& Spindler, M (2016). hdm: High-Dimensional Metrics R Journal, 8(2), 185-199.  \url{https://journal.r-project.org/archive/2016/RJ-2016-040/index.html}
  \item Friedman, J., Hastie, T., \& Tibshirani, R. (2001). The elements of statistical learning (Vol. 1, No. 10). New York: Springer series in statistics.
  \medskip
  \item James, G., Witten, D., Hastie, T., \& Tibshirani, R. (2013). An introduction to statistical learning (Vol. 112, p. 18). New York: springer.
  \medskip
  \item Zou, H. \& Hastie, T. (2005). Regularization and variable selection via the elastic net. Journal of the Royal StatisticalSociety, Series B.67: pp. 301–320
\end{itemize}

\end{frame}






%----------------------------------------------------------------------%
%----------------------------------------------------------------------%
\end{document}
%----------------------------------------------------------------------%
%----------------------------------------------------------------------%

